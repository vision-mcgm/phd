\documentclass[a4paper]{article}
\title{Characterising the visual system's representations of fire}
\author{Fintan S. Nagle}

\input{mylibrary}

\begin{document}
\maketitle


\section{Background}

Fire is a complex and dynamic phenomenon. Characterised by rapidly shifting patterns of colour and motion, it evokes rich visual percepts as well as aesthetic responses. Over the last 1.8 million years, the evolving human visual system has been exposed to a large amount of flamelike stimuli \cite{ bowman2009fire}whose perception, control and mastery conferred a key evolutionary advantage\cite{ bowman2009fire}.  We report an investigation of the visual features useful for discrimination between similar fires, the processes responsible for comparing them, and the specificity of the neural representations involved.



To engage the neural structures involved in fire perception, we used a delayed-match-to-sample task whose stimuli comprised short video clips of a hearth fire. In each trial, subjects viewed a sample clip followed by two slightly longer candidate clips, one of which was a padded version of the sample.

In such a 2-alternative forced choice task, evidence for either option is accumulated until a decision can (or must) be made\cite{bogacz2006physics}. We characterised the evidence present in fire clips by looking at the relationship between clip length and discrimination accuracy.

Fire clips provide different kinds of evidence: colour detail and luminance detail. Luminance-defined motion is processed better than colour-defined motion\cite{ramachandran1978does,cavanagh1984perceived,cabanagh1991contribution,mullen1992absence}, but colour-defined motion is nevertheless perceptible\cite{cropper1996rapid}. We measured the importance of colour-defined motion using monochromatic test stimuli.


Motion is categorised as either first- or second-order\cite{cavanagh1989motion,ledgeway1994evidence}. First-order motion is due to displaced form (the motion in time of correlations in space), whereas second-order motion is due to dynamic contours (motion of patterns which when still are not differentiable from the background).

Attention to these patterns of motion induces activation in separate neural structures\cite{ashida2007fmri,vaina1998selective}, implying that they may be processed differently.  A third type of motion percept, apparent movement, is induced by noncontinuous displacement (as in the $\phi$ and $\beta$ phenomena)\cite{steinman2000phi}. We altered the frame rates and temporal playback direction of video stimuli to examine the effets of different types of motion.

Object recognition is a key function of vision\cite{pelli2013object,dicarlo2012does} and is linked with outline shape\cite{hayward1998effects} because outlines delineate objects' spatial extent. To investigate whether object perception is important in fire perception, we measured subjects' discrimination accuracy on fire clips altered to show just object edges.

The visual system is specialised for certain types of stimuli, such as faces\cite{gauthier2001development,de2002specialization} or words\cite{cohen2004specialization}. Some stimuli provoke responses which differ in accuracy or response time\cite{fox2000facial}. We wondered whether the evolutionary benefit conferred by fire had specialised the human visual system. Expertise is commonly indicated by a performance boost when discriminating upright stimuli\cite{valentine1998upside}, and we found no evidence of an orientation-specific model for fire.



\section{Characterising the decision process}

To study the mechanisms at work in the perception of fire, we used a delayed match-to-sample 2AFC task involving short video clips of a hearth fire. In each trial, subjects viewed a sample clip followed by two slightly longer candidate clips, one of which contained the sample. The candidates were made longer due because the increased recognisability of start and end frames allowed matching to be done without attending to the rest of the sample. Subjects were asked to indicate in which candidate the sample was present, and their performance measured.

We used this matching paradigm in a series of experiments to characterise the comparison process and thus the representations involved. Matching is imperfect; we show that looking for a sample clip in longer test clips reduces performance. We then show that different types of visual information (colour, spatial form, and temporal order) are of very different importance in constructing representations of fire.

The 2AFC decision process can be seen as an accumulation of evidence supporting each choice\cite{bogacz2006physics}. While watching fire clips, subjects integrate evidence until a threshold is reached, allowing a decision to be made.

A percept which is present in one sample clip, but not the other, constitutes strong discriminatory evidence. If percepts from one sample are similar to those from another sample, the evidence they give is weaker and the two clips are more perceptually self-similar.


To investigate how self-similar fire clips appear, we varied the sample clip length in the matching paradigm. In Experiment 1 (see Fig. \ref{fig:e1}), sample length was varied (between blocks) between 10 and 50 frames (0.2 and 1 seconds). The ratio of sample length to candidate length was varied (within blocks) between 1.2 and 2 (candidates up to twice as long as targets).

Our prediction was that if fire clips were very differently perceived (individuated), a short sample clip would provide enough evidence to discriminate among either short or long test clips, and thus accuracy would not depend on test length. Conversely, if clips were perceived as self-similar, longer clips would induce more percepts which were erroneously matched with the sample (false evidence).

As shown in Fig. \ref{fig:e1}, it is candidate ratio (not candidate length) which has a highly significant negative effect on accuracy. It is not therefore the case that a unique, salient feature in the sample is being searched for in the test clips. There is high perceptual confusion between the sample and the false test. This means that fire clips are perceived as highly self-similar.

\section{Aspects useful to fire discrimination}

Whole-object representations, such as those found in human and macaque inferotemporal cortex\cite{kriegeskorte2008matching,baker2002impact,freedman2003comparison} are built from a range of restricted visual features such as object boundaries, colour, motion, texture and contour\cite{tsunoda2001complex,pasupathy1999responses}. We investigated the relative importance of these ingredients in the representation and comparison of fire. The same delayed-match-to-sample paradigm was employed, with an important difference: the sample was modified to remove some of its information. 

Consider showing a monochrome sample clip, with missing colour information, followed by two full-colour candidate clips. The representation set evoked by the sample is therefore lacking in colour information from the sample.

The candidate clips, on the other hand, are represented with their real colour. The process which compares candidates to samples must therefore match a detail-lacking representation to a detail-rich one. If the missing details are an important part of the representations, matching accuracy will suffer; however, the absence of unimportant details will not impact accuracy.

In Experiment 3 we removed three types of visual information from the samples: colour (by using monochrome clips), spatial arrangement (by rotating the samples through 180 degrees) and temporal arrangement (by playing the samples backwards). We measured the relative effect on accuracy of removing each detail.

Temporal inversion induced the greatest drop in accuracy (0.02, with 100\% accuracy expressed as 1) followed by reversed playback (0.04). Spatial inversion produced the greatest drop (0.11, or 11\%). (NB -STATISTICS WILL BE ADDED HERE)

We thus conclude that, in the set of representations accessible to the comparison process, colour is of much less importance than spatial and temporal form. The perception and recall of flame is therefore closely linked with the spatial arrangement, and the motion, of perceived image features.

\section{Object-based representations}

Some forms of incoming visual information is easily segmented into objects, such as faces, bodies and foodstuffs\cite{kriegeskorte2008matching}. Object-centred representations allow abstraction away from retinotopic position\cite{pasupathy2001shape} and efficient coding as object files\cite{treisman1993perception,kahneman1992reviewing}. We wondered whether object representations of flames are important for fire recognition. 

Objects are delimited by their edges, which describe their shape. Texture and colour information remains inside the outlined area. Recognition proceeds very well using just their edges or outlines, as shown by the easy recognition of faces from line drawings\cite{bruce1992importance} and the drop in processing required to interpret line drawings\cite{leder1996line}. We therefore hypothesised that if object recognition was important in fire discrimination, unaltered fire clips would be matched with edges-only fire clips with high accuracy. Conversely, if textural features occuring inside objects were more important, this type of cross-matching would greatly impair performance.

We repeated the altered-sample matching task, employing a Sobel edge-detection filter\cite{kittler1983accuracy} to preserve only pixels with the highest spatial derivative. This filter extracted the edges of flames very well, replacing their interior with a black background.

Discrimination accuracy dropped from x to x (p=x), showing that representations built from edges alone were matched accurately with fullly-informed representations. Edge information is therefore very important for fire perception. Although we present no evidence for the existence of object files or nonretinotopic representations, the widespread ease of object recognition from outlines suggests that object files may be important for fire discrimination.



\section{Motion information}


Experiment 4 showed a series of clips and required a "forwards-or-backwards" judgement after each clip. Clips varied in frame rate, with interframe delays ranging from 0.02 s (50 fps, the original frame rate) to 0.2 s (5 fps). Clips were also shown rotated by 0, 90, 180 or 270 degrees (Fig. \ref{f5}). Clip speed or length was not varied.

The lower we set the frame rate, the less motion information subjects could obtain from clips. Reducing frame rate degraded first- and second-order motion equally, while leaving motion percepts due to apparent motion intact.

Mean accuracy was 90\% at full frame rate, but descended quickly to chance at 10 fps (interframe delay: 0.1 s). First- and second-order motion information was thus extremely important to fire perception.

Below 10 fps, any apparent motion perceived in the image is not useful for discrimination. This may be because forms in the image change shape so much over a tenth of a second that they cannot be equated, leading to no apparent motion.

Experiment (edges), by constructing edge-filtered clips, removed all second-order motion information. The small drop in performance revealed that second-order motion was not very useful for fire discrimination.

We conclude that first-order motion is of great importance in the representation and discrimination of fire.

\section{Specialisation by direction tuning}

Certain local neural codes are specialised: they are adapted to representing specific stimulus classes. For example, we can easily read out the presence or absence of a face by monitoring face cells in STS\cite{tsao2008patches}. Some representations are tuned to stimuli oriented in a particular direction.

In face recognition, the improved recognition of upright faces over inverted faces is termed the inversion effect\cite{valentine1988upside}. Although there is debate about its exact causes\cite{farah1995causes}, it is generally accepted as evidence of specialisation for face processing. The representation set which informs face recognition is better tuned to encoding upright faces, and thus it cannot encode inverted faces as effectively. This impairs recognition.

The face inversion effect is present in delayed match-to-sample 2AFC tasks\cite{yin1969looking}. We checked for the presence of a similar effect in the task of fire discrimination, using two experiments. 

Firstly, Experiment 3 used the same delayed-match-to-sample paradigm as Experiment 1, with one factor only: in half the trials, the sample and candidate clips were both inverted. This tested the ability of the brain's representation set to store and evaluate codings of inverted fire clips. As shown in Fig \ref{f5}, there was no significant difference in recognition rates between upright and inverted clips. In fact, 50\% of subjects performed slightly better with inverted clips.

Secondly, we looked at recognition accuracies across the four rotation directions used in Experiment 4: upright, inverted, and rotated 90 degrees to the left and right. As shown in Fig \ref{f5}, there was once again no significant difference in accuracy across the rotation directions. 

There is no large, decisive performance impairment with fire stimuli; inversion does not impair fire recognition.

This could be explained by fire representations which were direction-independent. With faces, the inversion effect seems to be due to mis-coding of configural information (faces which differ in feature configuration are recognised worse under inversion than faces which differ in feature identity\cite{freire2000face}); this information is highly vertically asymmetric, meaning that flipped configurations are badly encoded. With flames, the lack of an inversion effect indicates that the brain is as good at representing inverted fires as upright ones; its representations are symmetric. 

If the representations available to the fire comparison process were better at coding a specific direction of motion (as we could imagine, given that flaming gas usually moves upward), discrimination would suffer under rotation. However, as these encodings store enough information to allow differentiation of clips rotated by 180 degrees or by 90 degrees in either direction, we conclude that they not tuned specifically to motion in any direction.


\section{Discussion}



\section{Conclusions}



\section{Materials and Methods}

Stimuli used in these experiments were recorded from a hearth fire under a mixture of natural and artificial lighting. Video was captured 50 Hz using a Sony INS camera with zero CCD gain. Final cropped clips were 641 pixels high by 564 pixels wide.

Stimuli were displayed, in a darkened room, on a INS monitor with a refresh rate of 100 Hz, driven by a standard graphics card. Subjects used a chin-rest at a distance of INS and were asked not to deviate their head angle from the vertical. They were not requested to fixate.

\subsection{Experiment 1}

Subjects were 12 UCL undergraduates with normal (or corrected) vision.

A delayed match-to-sample paradigm was used. On each trial, subjects viewed three sequential video clips of flames separated by an ISI of INS: one sample, followed by two test clips. Test clips were equal in length and longer than the sample. One test consisted of the sample, with short sections of video before and after it (in this clip, the sample's start position was random). The other test was an equal-length clip from a random part of the data set which did not contain the sample.

Sample lengths consisted of 10, 25 and 50 frames (0.2, 05 and 1 s). For each sample length, test clip lengths were set at 1.2, 1.4, 1.6, 1.8 and 2 times sample length. Sample length was kept constant within blocks, during which test length was varied.



\subsection{Experiment 2}

Subjects were 12 UCL undergraduates with normal (or corrected) vision.

The same delayed match-to-sample protocol was used. Here, sample lengths were 1, 3, 6 and 12 frames (0.02, 0.06, 0.12 and 0.24 s). Test lengths were 15, 20 and 40 frames (0.3, 0.4 and 0.8 s). Test length was kept constant within blocks, during which sample length was varied.



\subsection{Experiment 3}

The same delayed match-to-sample protocol was used. Sample clips had a constant length of 10 frames (0.2 s) and test clips a constant length of 15 frames (0.3 s).

Test clips were displayed normally. Sample clips were altered in one of three ways.
	a) Colour manipulation: in hue-saturation-value (HSV) space, clips were rotated 180\degree around the hue axis.
	b) Spatial manipulation: clips were played upside down
	c) Temporal manipulation: clips were played backwards

Sample manipulation was varied between blocks. Each alteration was presented across 4 separate blocks, along with 4 control blocks whose samples were unmodified. The order of the set of 16 blocks was randomised.



\subsection{Experiment 4}

On each trial, one clip was shown. Subjects were then asked to judge whether it had been played forwards or backwards.

Clip lengths were 100 frames (2 s). Clip frame rate (not length or speed) was manipulated: interframe delays were 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18, and 0.2 s (corresponding to frame rates of 50, 25, 16.7, 12.5, 10, 8.3, 7.14, 6.3, 5.6 and 5 Hz).

Orientation was also manipulated: clips were shown at either 0\degree, 90\degree, 180\degree or 270\degree. 

Clip length was varied within blocks, whereas orientation was varied across blocks, with 2 blocks per angle.  Block order was randomised. Subjects were 14 UCL undergraduates.

%FIGURES LIST
%f1 		modelling		f0
%f2		test-vary		f1
%f3		sample-vary	no AI
%f4		manipulations	f4.ai
%f5		f5.ai


  \begin{figure}[!ht]
\centering
  FIGURE 1
    \caption{A: three progressively more detailed ways of modelling the fire comparison process. B: typical natural fire. C: typical computer-generated fire. D: one of the stimuli used in this study.}
    \label{f1}
  \end{figure}

  \begin{figure}[!ht]
\centering
  FIGURE 2
    \caption{A: one sample clip was shown before two test clips, one of which was the sample (padded). B: For each sample length, five test lengths were used. C: classification accuracies for the 3 sample lengths. D: accuracy appears to depend on }
    \label{f2}
  \end{figure}

  \begin{figure}[!ht]
\centering
  FIGURE 3
    \caption{On the same 2AFC task (matching a sample to one of two test clips), accuracy was measured as sample length was varied. Accuracy increased with sample length.}
    \label{f1}
  \end{figure}

  \begin{figure}[!ht]
\centering
  FIGURE 4
    \caption{Experiment 3. A: in a delayed-match-to-sample task, sample clips were altered by colour inversion, temporal inversion, and spatial inversion. This disrupted colour information, temporal order of features and spatial layout of features. B: subjects needed to match a stored, altered percept with an incoming, unaltered clip. C: decreases in accuracy showed which types of information were most important for matching: spatial configuration, followed by temporal order.}
    \label{f4}
  \end{figure}

  \begin{figure}[!ht]
\centering
  FIGURE 5
    \caption{Experiment 4. A: Subjects were asked to indicate whether clips were played forwards or backwards. This task requires an internal model of what real fire looks like, and the ability to compare it to incoming stimuli. B: Subjects' accuracy showed no significant effect of orientation. C: accuracy declined quickly to chance when frames were separated by 0.1 seconds or more. We conclude that subjects make use of short-range correlations between frames, and that long-range correlations are either not represented or not useful.}
    \label{f5}
  \end{figure}


\begin{singlespace}
\begin{footnotesize}
\begin{twocolumn}
\bibliographystyle{unsrt}
\bibliography{references,/Users/fintan/Dropbox/MasterReferences}
\end{twocolumn}
\end{footnotesize}




\end{singlespace}
\newpage


\end{document}




