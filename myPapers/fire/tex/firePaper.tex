\documentclass[a4paper]{article}
\title{Characterising the visual system's representations of fire}
\author{Fintan S. Nagle}

\input{mylibrary}

\begin{document}
\maketitle


\section{Background}


Fire is a complex and dynamic phenomenon. Characterised by rapidly shifting patterns of both first- and second-order motion, it evokes rich visual percepts as well as aesthetic responses. We report an investigation of the visual features useful for discrimination between similar fires, the processes responsible for comparing them, and the specificity of the neural representations involved.

\subsection*{Fire: a venerable visual stimulus}

From an evolutionary point of view, mastery of fire was key to human development. Being able to control fire allowed early humans to cook food, defend themselves from predators and survive in cold, challenging environments. Fire was the first of a long line of technologies which release stored energy from fuel and turn it to human purposes; the earliest archaeological evidence of fire use dates back 1.8 million years, with frequent use found from 100,000 years ago\cite{bowman2009fire}. Even before this, hominids regularly encountered flame in the form of bushfires, although these were perceived as a threat, not a controllable, exploitable entity.

The evolving human visual system has therefore been exposed to a large amount of flamelike stimuli in the last 1.8 million years. These stimuli have often appeared in dangerous or life-threatening contexts, either posing a threat or aiding survival. In sufficiently extreme situations, such as extreme cold or heavy predation, those early humans who could successfully control fire had an increased chance of survival.

It is therefore natural to enquire whether the human visual system has become adapted in any way to the perception of flamelike stimuli. Does the visual system employ any specific representations or specialised models when attending to fire, does it use the same general-purpose systems employed when observing a novel moving stimulus?

This question recalls the ongoing debate concerning the specialisation of face perception. We find increased activation of the fusiform face area and inferior temporal sulcus while viewing faces\cite{allison2000social}; this can be explained either by innate specialisation or learned proficiency. In the same way, observation of fire may recruit neurons and systems which respond preferentially to, and perform better on, flame stimuli. On the other hand, observing fire may stimulate the same neural populations as observing other moving stimuli.

This report aims to answer two questions. Firstly, what types of visual information are important when observing flames; which details are represented, and which thrown away? This choice defines the upstream visual subsystems (for example, that of motion detection, edge detection or shape detection) which are activated by fire stimuli. Secondly, are any of these systems specific to fire? We do not address the question of whether any specificity is learned or innate.

We begin with a survey of the ecological context in which fire is observed and processed. We then describe several yes/no and 2AFC tasks designed to activate the visual subsystems which process fire and measure task performance under removal of different types of visual information from the stimulus.

\section{Fire in the natural environment}



%We can define the set of dynamic visual stimuli termed "fire" as the light fields emitted from combusting objects. In terms of luminance, they display rapid variation from gentle glows to dazzling flares, which covers virtually the whole photopic range of the visual system; one can imagine that scotopic vision is not employed except for very dim, distant fires. In terms of size, they may subtend any angle from the whole visual field (for a large, close fire) to a point (for a small, distant fire).
%
%In terms of global spatial frequency, fire is characterised by 
%
%FFT stuff here

The early human visual system processed and represented fire during two tasks: a) avoiding large natural wildfires, and b) controlling small artificial hearth fires for cooking and defence. Good performance at both tasks is essential for survival, but we are not as interested in visual development supporting task performance in situation a), as it is not specific to humans; all large land animals in an environment at risk of wildfires must possess this skill. We consider only the anthrospecific task of controlling small artificial fires.

Fire is employed in many situations, from defensive to culinary to ritualistic. In order to avoid being overly selective, we do not consider a particular task, but the general context in which they can all be placed: the extraction of useful information (or affordance) from visual stimuli. In order to properly control a fire, the observer must estimate properties such as the temperature of various parts, whether the fire is growing or shrinking, whether it is likely to spark or flare up, and in which direction it will spread. Knowing these details accurately is especially important in the early stages of firelighting, when the flame is small and at risk of extinguishing.

We therefore restrict this work to the study of small, artificial fires which do not pose an imminent threat to survival. 

\section{How is fire represented?}

%We can consider a visual stimulus to be coded using a large set of varyingly structured representations, varying in their cortical location and information content.

We investigated flame perception psychophysically using a delayed match-to-sample task involving short video clips of a hearth fire. In each trial, subjects viewed a sample clip followed by two slightly longer candidate clips, one of which contained the sample. The candidates were made longer due because the increased recognisability of start and end frames allowed matching to be done without attending to the rest of the sample. Subjects were asked to indicate in which candidate the sample was present, and their performance measured.

The cognitive process being tested was thus one of perception and comparison between a sample and two candidates. Although this task is not a perfect analogue of the ecological contexts in which fire is perceived (which involve evaluation of one fire, rather than comparison of two), it allows us to characterise the neural representations involved, as they play a key part in the discrimination process.

We have data corresponding to the start point of this neural process (the stimulus) and the end point (the yes/no comparison). We can start by modelling the process as a black box labelled ``processing" (Fig \ref{f1}). A possible more detailed model distinguishes the processes of ``representation" and ``comparison" (Fig \ref{f1}), which occur sequentially; however, we do not know whether these processes can actually be distinguished.

We used this matching paradigm in a series of experiments to characterise the comparison process and thus the representations involved. Matching is imperfect; we show that looking for a sample clip in longer test clips reduces performance. We then show that different types of visual information (colour, spatial form, and temporal order) are of very different importance in constructing representations of fire.



\section{Characterising the comparison process}

We can see the process of neural coding as a function mapping visual inputs to neural states. It is useful to divide the overall neural state into separate representations (as in dorsal/ventral visual stream theory).

The task as a whole can be seen as a process, with memory, which maps neural states to a binary response (the yes/no signal recorded by the experimental setup). Comparison involves task-oriented function and finally motor control.

\subsubsection{Cross-correlation?}

One characteristic of the comparison process is easy to determine psychophysically: whether or not it is analogous to cross-correlation, a process via which one small sample pattern is "slid along" or "moved over" a large candidate pattern (usually an image, signal or matrix). As it moves, the small pattern is compared locally to the large pattern. Cross-correlation is characterised by independence from candidate pattern size: as the comparison is local, any matching areas of the candidate pattern will be detected even if it is very large.

We looked for this size-independence property in the fire comparison task by varying the candidate clip length in the matching paradigm. In Experiment 1 (see Fig. \ref{fig:e1}), sample length was varied (between blocks) between 10 and 50 frames (0.2 and 1 seconds). The ratio of sample length to candidate length was varied (within blocks) between 1.2 and 2 (candidates up to twice as long as targets).

We predicted that, if comparison was being done by a process analogous to cross-correlation, samples would match up equally well against short candidates and long candidates. The matching region would be detected by local comparison. Therefore, accuracy would not depend on candidate length. On the other hand, if the comparison process were more global, longer candidates would need to fit into a fixed-information representation. This would lead to information loss and less efficient matching.

\subsubsection*{Ruling out the effect of confusability}

As shown in Fig. \ref{fig:e1}, candidate ratio has a highly significant negative effect on accuracy. Apart from comparison dynamics (whether or not comparison is cross-correlation-esque), there is another consideration which could produce this effect: stimulus confusability.

Because the comparison process is not perfect, it is possible for areas in the candidate to be erroneously matched to the target (confused). Because fire is a repetitive, self-similar stimulus, the longer the candidate, the higher the likelihood that confusable areas will appear in the target. This, as opposed to global representation, could cause the negative effect of candidate ratio on accuracy.

To rule this out, we investigated the effect of varying sample length on performance (Experiment 2). If the stimulus was highly confusable, longer samples would stand a higher chance of being falsely matched with candidates, which would decrease accuracy. If not, longer samples would provide more useful information, increasing accuracy.

Experiment 2 followed the same match-candidates-to-sample paradigm. This time, sample length was varied (within blocks) between 1 and 12 frames (0.02 and 0.24 seconds), while target length was varied (across blocks) between 15 and 20 frames (0.3 and 0.4 seconds).

As shown in Fig. \ref{fig:e1}, sample length had a highly significant positive effect on accuracy; longer samples were easier to match. If longer samples were more confusable due to self-similarity, a negative effect of sample length on accuracy would have been expected.

In a delayed-match-to-sample task, sample length has a positive effect on subject accuracy, whereas candidate length has a negative effect. If the latter effect were due to  higher confusability between samples and longer candidates, we would not have seen the former effect. We thus conclude that the neural comparison process is not analogous to convolution, and that there are constraints in place which make it less effective on longer candidates.

These constraints may stem from the nature of the representations involved: global encodings may hold the same amount of information about the stimulus, independent of its length. This would lead to more information loss for longer candidate stimuli, and thus a loss of accuracy.

Constraints may also be temporal: even given perfect representations, the process which compares them may be time-limited for efficiency. Comparing perceived objects to representations is a core, often time-critical visual task. The comparison process may therefore permit more information loss, and lower accuracy, when acting on longer representations of larger clips.

%(We could test this by not giving much time to perform the comparison). 

%Re-read up to here

\subsubsection*{Accuracy depends on length ratio}

As shown in Fig. \ref{f2}, accuracy depends not on the absolute length of the test clips, but on the ratio of test length to sample length. This indicates that there is a loss of information both in the sample representations and test representations. 

\section{What aspects of fire do we represent?}

Representations are built from a range of visual features such as object boundaries, colour, motion, texture and contour. We investigated the relative importance of these ingredients in the representation and comparison of fire. The same delayed-match-to-sample paradigm was employed, with an important difference: the sample was modified to remove some of its information. 

Consider showing a monochrome sample clip, with missing colour information, followed by two full-colour candidate clips. The representation set evoked by the sample is therefore lacking in colour information from the sample (in specialised representations, colour may be filled in according to an internal model, but will not correspond to the sample's actual colour).

The candidate clips, on the other hand, are represented with their real colour. The process which compares candidates to samples must therefore match a detail-lacking representation to a detail-rich one. If the missing details are an important part of the representations, matching accuracy will suffer; however, the absence of unimportant details will not impact accuracy.

In Experiment 3 we removed three types of visual information from the samples: colour (by using monochrome clips), spatial arrangement (by rotating the samples through 180 degrees) and temporal arrangement (by playing the samples backwards). We measured the relative effect on accuracy of removing each detail.



Temporal inversion induced the greatest drop in accuracy (0.02, with 100\% accuracy expressed as 1) followed by reversed playback (0.04). Spatial inversion produced the greatest drop (0.11, or 11\%).

We thus conclude that, in the set of representations accessible to the comparison process, colour is of much less importance than spatial and temporal form. The perception and recall of flame is therefore closely linked with the spatial arrangement, and the motion, of perceived image features.

%
%  \begin{figure}[!ht]
%\centering
%    \subfloat[To alter their natural colour, clips were processed by expression in HSV colour space and rotation through 180\degree in the hue plane. Clips were also inverted. \label{subfig-2:dummy}]{%
%      \includegraphics[width=\textwidth]{img/manipulations.png}
%    }
%\hspace{0.3cm}
%
%\subfloat[The delayed match-to-sample paradigm was reapplied, with alterations (conversion to monochrome, reversed playback, and inverted playback) made to the sample only. Encodings of the sample, with partial information, thus had to be matched to unaltered candidates. Error bars show 95\% confidence intervals.\label{subfig-2:dummy}]{%
%      \includegraphics[width=\textwidth]{img/accuracyvsmanipulation.pdf}
%    }
%    \caption{\textbf{Experiment 3.} In a delayed-match-to-sample task, sample clips were altered by colour inversion, temporal inversion, and spatial inversion. This disrupted colour information, temporal order of features and spatial layout of features. Subjects needed to match a stored, altered percept with an incoming, unaltered clip. Decreases in accuracy showed which types of information were most important for matching: spatial configuration, followed by temporal order.}
%    \label{fig:dummy}
%  \end{figure}



\section{Are these representations specialised?}

Certain local neural codes are specialised: they are adapted to representing specific stimulus classes. For example, we can easily read out the presence or absence of a face by monitoring face cells in STS\cite{tsao2008patches}.

The question of whether specialised codes exist for a certain stimulus group is very similar to that of whether an internal model of that group exists. An internal model forms specialised representations and, centrally, fills them in: it can fill in or interpolate absent details from given information. For example, on perceiving the bottom half of a face, one can often imagine the appearance of the upper half.

There is no question that we have a basic internal model of fire, because we can recognise it trivially and make simple judgements about its affordances and size. This model is also not perfect, because we cannot predict in detail what a fire is going to do (such a skill would require simulating a complex thermodynamic system, and immense processing power). We can, however, attempt to measure how good our model is.

Models are used to make judgements from stimuli. For example, expertise in face perception serves to guess at a conspecific's age, physical condition and current mental state. The better the model, the more accurate these judgements and the less information required to make them.

 We investigated the effect of reduced information on accuracy of a particular judgement of hearth fire clips. Given the importance of temporal order to fire perception, we asked participants to judge whether clips were being played forwards or backwards.

Whether played forwards or backwards, a clip contains the same sequence of frames and causes the same patterns of light to fall upon the retina. What differs is order. Being presented in reverse, these frames evoke opposite-direction motion percepts.

Our predictions were as follows. Consider a subject with a highly-developed model of flame. Normal clips would register well with this model, being perceived as real. Clips played in reverse, however, would evoke motion percepts which did not tally with those expected by the model, leading to a sensation of irreality.

Experiment 4 tested these predictions by showing a series of clips and requesting a "forwards-or-backwards" judgement after each clip. Clips varied in frame rate, with interframe delays ranging from 0.02 s (50 fps, the original frame rate) to 0.2 s (5 fps). Clips were also shown rotated by 0, 90, 180 or 270 degrees (Fig. \ref{f5}). 

Mean accuracy was 90\% at full frame rate, but descended quickly to chance at 10 fps (interframe delay: 0.1 s). Subjects' internal models of fire were good at predicting reality when frames were close together, but found this task impossible with widely spaced frames.

This drop in accuracy means that the correlations encoded by the model are mostly short-range, concerning the relationships between frames close together in time. These representations of fire store local descriptions.

Skill at this task was already present, not acquired during the experiment. We plotted accuracy against progress through the task; performance did not significantly increase between the first and last quarters of the experiment. Forwards/backwards judgement therefore uses existing representations and processes, not new skills learned during the task.


\section{How local are fire encodings?}

%  \begin{figure}[!ht]
%\centering
%    \subfloat[To investigate subjects' ability to make judgements using their internal model of flames, a forwards/backwards detection task was used. During each trial, one short clip was played either forwards or backwards. Subjects were asked to decide which using a binary response. \label{subfig-2:dummy}]{%
%      \includegraphics[width=0.6\textwidth]{img/protocol2.png}
%    }
%\hspace{0.3cm}
%
%\subfloat[During the forwards/backwards detection task, clip frame rates were varied between 50 and 5 frames per second (interframe delays of between 1 and 15 times the normal frame delay of 0.02 seconds). Accuracy declined quickly, reaching chance at a frame delay of 5 times normal (0.1 seconds). \label{subfig-2:dummy}]{%
%      \includegraphics[width=0.45\textwidth]{img/accuracy.pdf}
%    }
%\hspace{0.3cm}
%\subfloat[The data from (b) are shown on a linear-logarithmic plot. The good straight line fit indicates that the accuracy-delay relationship is exponential. \label{subfig-2:dummy}]{%
%      \includegraphics[width=0.45\textwidth]{img/accuracylog.pdf}
%    }
%
%    \caption{\textbf{Experiment 4.} Subjects were asked to discriminate clips played forwards and those played in reverse. This task requires an internal model of what real fire looks like, and the ability to compare it to incoming stimuli. Subjects' accuracy declined quickly to chance when frames were separated by 0.1 seconds or more. We conclude that subjects make use of short-range correlations between frames, and that long-range correlations are either not represented}
%    \label{fig:dummy}
%  \end{figure}


\section{Are fire representations tuned to direction?}

Some representations are tuned to stimuli oriented in a particular direction.

In face recognition, the improved recognition of upright faces over inverted faces is termed the inversion effect. Although there is debate about its exact causes\cite{farah1995causes}, it is generally accepted as evidence of specialisation for face processing. The representation set which informs face recognition is better tuned to encoding upright faces, and thus it cannot encode inverted faces as effectively. This impairs recognition.

We checked for the presence of a similar effect in the task of fire discrimination, using two experiments. 

Firstly, Experiment 3 used the same delayed-match-to-sample paradigm as Experiment 1, with one factor only: in half the trials, the sample and candidate clips were both inverted. This tested the ability of the brain's representation set to store and evaluate codings of inverted fire clips. As shown in Fig \ref{f5}, there was no significant difference in recognition rates between upright and inverted clips. In fact, 50\% of subjects performed slightly better with inverted clips.

Secondly, we looked at recognition accuracies across the four rotation directions used in Experiment 4: upright, inverted, and rotated 90 degrees to the left and right. As shown in Fig \ref{f5}, there was once again no significant difference in accuracy across the rotation directions. 

In face recognition tasks, it is commonplace to find an accuracy drop of over 10\% with inverted faces\cite{freire2000face,yovel2005neural}. This large, decisive effect is not present with fire stimuli; inversion does not impair fire recognition as much as flame recognition.

This could be explained by fire representations which were direction-independent. With faces, the inversion effect seems to be due to mis-coding of configural information (faces which differ in feature configuration are recognised worse under inversion than faces which differ in feature identity\cite{freire2000face}); this information is highly vertically asymmetric, meaning that flipped configurations are badly encoded. With flames, the lack of an inversion effect indicates that the brain is as good at representing inverted fires as upright ones; its representations are symmetric. 

If the representations available to the fire comparison process were better at coding a specific direction of motion (as we could imagine, given that flaming gas usually moves upward), discrimination would suffer under rotation. However, as these encodings store enough information to allow differentiation of clips rotated by 180 degrees or by 90 degrees in either direction, we conclude that they not tuned specifically to motion in any direction.

\section{Summary}

\begin{itemise}
\item Fire stimuli are confusable, with longer clips leading to more false positives.
\item Comparison is not done by a sliding-window process.
\item Spatial information is most important when discriminating fire; colour and temporal form information less so.
\item Humans can reliably discriminate forwards-moving fire from backwards-moving fire.
\item The internal model of fire is not orientation-specific.


\end{itemise}

\section{Discussion}

Initial investigations of the cognitive processes which evaluate and perceive fire indicate that local correlations are represented well (NOTE direction exp), but also that longer clips provide more local detail which clouds discrimination (NOTE first 2 exps). This suggests that visually perceived fires are encoded as a bag of spatially and temporally local features.

Larger bags of features impair recognition. This could be because the local feature representations are used to generate a fixed-information global representation, which can only effectively store a certain amount of detail. Alternatively, it could be because more local representations are more costly to compare, and the matching process loses accuracy to minimise cost.



Its presence in the environment of the evolving 


We are thinking on a level between the neural and the cognitive, a level insufficiently detailed to model individual neurons but more specific than descriptions of qualia or percepts. Our main primitive is the concept of the \textit{neural representation}, a description of how a particular idea, metric or concept is coded by neural activity or architecture. It is illustrative to provide some examples.


A place cell, which fires when an animal occupies a particular area of its environment, is a representation of that particular location. Simiarly, a head direction cell represents orientation.

Visual information is encoded in many different representations as it passes from retina to cortex.


\section{Materials and Methods}

Footage used in these experiments was recorded from a hearth fire under a mixture of natural and artificial lighting. Video was captured 50 Hz using a Sony INS camera with zero CCD gain. Final cropped clips were 641 pixels high by 564 pixels wide.

Stimuli were displayed, in a darkened room, on a INS monitor with a refresh rate of 100 Hz, driven by a standard graphics card. Subjects used a chin-rest at a distance of INS and were asked not to deviate their head angle from the vertical. They were not requested to fixate.

\subsection{Experiment 1}

A delayed match-to-sample paradigm was used. On each trial, subjects viewed three sequential video clips of flames separated by an ISI of INS: one sample, followed by two test clips. Test clips were equal in length and longer than the sample. One test consisted of the sample, with short sections of video before and after it (in this clip, the sample's start position was random). The other test was an equal-length clip from a random part of the data set which did not contain the sample.

Sample lengths used were 10, 25 and 50 frames (0.2, 05 and 1 s). For each sample length, test clip lengths were set at 1.2, 1.4, 1.6, 1.8 and 2 times sample length. Sample length was kept constant within blocks, during which test length was varied.

Subjects were 12 UCL undergraduates with normal (or corrected) vision.

\subsection{Experiment 2}

The same delayed match-to-sample protocol was used. Trials were as in Experiment 1. Here, sample lengths were 1, 3, 6 and 12 frames (0.02, 0.06, 0.12 and 0.24 s). Test lengths were 15, 20 and 40 frames (0.3, 0.4 and 0.8 s). Test length was kept constant within blocks, during which sample length was varied.

Subjects were 12 UCL undergraduates with normal (or corrected) vision.

\subsection{Experiment 3}

The same delayed match-to-sample protocol was used. Sample clips had a constant length of 10 frames (0.2 s) and test clips a constant length of 15 frames (0.3 s).

Test clips were displayed normally. Sample clips were altered in one of three ways.
	a) Colour manipulation: in hue-saturation-value (HSV) space, clips were rotated 180\degree around the hue axis.
	b) Spatial manipulation: clips were played upside down
	c) Temporal manipulation: clips were played backwards

Sample manipulation was varied between blocks. Each alteration was presented across 4 separate blocks, along with 4 control blocks whose samples were unmodified. The order of the set of 16 blocks was randomised.

Subjects (4 UCL undergraduates) thus had to find correspondences between altered samples and real tests.


\subsection{Experiment 4}

On each trial, one clip was shown. Subjects were then asked to judge whether it had been played forwards or backwards.

Clip lengths were 100 frames (2 s). Clip frame rate (not length or speed) was manipulated: interframe delays were 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18, and 0.2 s (corresponding to frame rates of 50, 25, 16.7, 12.5, 10, 8.3, 7.14, 6.3, 5.6 and 5 Hz).

Orientation was also manipulated: clips were shown at either 0\degree, 90\degree, 180\degree or 270\degree. 

Clip length was varied within blocks, whereas orientation was varied across blocks, with 2 blocks per angle.  Block order was randomised. Subjects were 14 UCL undergraduates.

%FIGURES LIST
%f1 		modelling		f0
%f2		test-vary		f1
%f3		sample-vary	no AI
%f4		manipulations	f4.ai
%f5		f5.ai


  \begin{figure}[!ht]
\centering
  FIGURE 1
    \caption{A: three progressively more detailed ways of modelling the fire comparison process. B: typical natural fire. C: typical computer-generated fire. D: one of the stimuli used in this study.}
    \label{f1}
  \end{figure}

  \begin{figure}[!ht]
\centering
  FIGURE 2
    \caption{A: one sample clip was shown before two test clips, one of which was the sample (padded). B: For each sample length, five test lengths were used. C: classification accuracies for the 3 sample lengths. D: accuracy appears to depend on }
    \label{f2}
  \end{figure}

  \begin{figure}[!ht]
\centering
  FIGURE 3
    \caption{On the same 2AFC task (matching a sample to one of two test clips), accuracy was measured as sample length was varied. Accuracy increased with sample length.}
    \label{f1}
  \end{figure}

  \begin{figure}[!ht]
\centering
  FIGURE 4
    \caption{Experiment 3. A: in a delayed-match-to-sample task, sample clips were altered by colour inversion, temporal inversion, and spatial inversion. This disrupted colour information, temporal order of features and spatial layout of features. B: subjects needed to match a stored, altered percept with an incoming, unaltered clip. C: decreases in accuracy showed which types of information were most important for matching: spatial configuration, followed by temporal order.}
    \label{f4}
  \end{figure}

  \begin{figure}[!ht]
\centering
  FIGURE 5
    \caption{Experiment 4. A: Subjects were asked to indicate whether clips were played forwards or backwards. This task requires an internal model of what real fire looks like, and the ability to compare it to incoming stimuli. B: Subjects' accuracy showed no significant effect of orientation. C: accuracy declined quickly to chance when frames were separated by 0.1 seconds or more. We conclude that subjects make use of short-range correlations between frames, and that long-range correlations are either not represented or not useful.}
    \label{f5}
  \end{figure}


\begin{singlespace}
\begin{footnotesize}
\begin{twocolumn}
\bibliographystyle{unsrt}
\bibliography{references}
\end{twocolumn}
\end{footnotesize}
\end{singlespace}
\newpage


\end{document}




